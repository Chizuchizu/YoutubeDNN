{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import glob\n",
    "import MeCab\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 10, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# 記事ベクトルの作成\n",
    "\n",
    "np.random.seed(0)\n",
    "text_paths = glob.glob('livedoor_news_corpus/text/**/*.txt')\n",
    "# print(len(text_paths))  # 7376\n",
    "model = gensim.models.Word2Vec.load('ja.bin')\n",
    "\n",
    "\n",
    "def analyzer(text, mecab, stopwords=[], target_part_of_speech=['proper_noun', 'noun', 'verb', 'adjective']):\n",
    "    node = mecab.parseToNode(text)\n",
    "    words = []\n",
    "    while node:\n",
    "        features = node.feature.split(',')\n",
    "        surface = features[6]\n",
    "        if (surface == '*') or (len(surface) < 2) or (surface in stopwords):\n",
    "            node = node.next\n",
    "            continue\n",
    "        noun_flag = (features[0] == '名詞')\n",
    "        proper_noun_flag = (features[0] == '名詞') & (features[1] == '固有名詞')\n",
    "        verb_flag = (features[0] == '動詞') & (features[1] == '自立')\n",
    "        adjective_flag = (features[0] == '形容詞') & (features[1] == '自立')\n",
    "        if ('proper_noun' in target_part_of_speech) & proper_noun_flag:\n",
    "            words.append(surface)\n",
    "        elif ('noun' in target_part_of_speech) & noun_flag:\n",
    "            words.append(surface)\n",
    "        elif ('verb' in target_part_of_speech) & verb_flag:\n",
    "            words.append(surface)\n",
    "        elif ('adjective' in target_part_of_speech) & adjective_flag:\n",
    "            words.append(surface)\n",
    "        node = node.next\n",
    "    return words\n",
    "\n",
    "\n",
    "req = urllib.request.Request('http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt')\n",
    "with urllib.request.urlopen(req) as res:\n",
    "    stopwords = res.read().decode('utf-8').split('\\r\\n')\n",
    "while '' in stopwords:\n",
    "    stopwords.remove('')\n",
    "\n",
    "words_list = []\n",
    "for tp in text_paths[:15]:\n",
    "    text = open(tp, 'r').read()\n",
    "    text = text.split('\\n')\n",
    "    # title = text[2]\n",
    "    text = ' '.join(text[3:])\n",
    "    mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    words = analyzer(text, mecab, stopwords=stopwords, target_part_of_speech=['noun', 'proper_noun'])\n",
    "    words = filter(lambda x: x in model.wv.vocab, words)\n",
    "    words_list.append(' '.join(words))\n",
    "\n",
    "docs = np.asarray(words_list)\n",
    "count = CountVectorizer()\n",
    "bags = count.fit_transform(docs)\n",
    "bags.toarray()  # (2, N)  全単語の出現回数 0を含む\n",
    "features = count.get_feature_names()\n",
    "\n",
    "# features: データ内の全単語 (F)\n",
    "# docs: 各文章の単語を' 'でつなげたもの\n",
    "# bags: featuresのidxにしたがって, 出現回数をカウントしたもの (N, F)\n",
    "# \n",
    "# model.wv[words_list[0]]\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "tf_idf = tfidf.fit_transform(bags)\n",
    "tf_idf = tf_idf.toarray()\n",
    "# print(np.take_along_axis(tf_idf, tf_idf.argsort(axis=1), axis=1))\n",
    "features = np.asarray(features)\n",
    "sorted_features = features[tf_idf.argsort(axis=1)][:, -10:]  # 各item内におけるtf-idf top10の単語\n",
    "\n",
    "item_vecs = []\n",
    "for f in sorted_features:\n",
    "    item_vecs.append(model[f])\n",
    "print(np.asarray(item_vecs).shape)\n",
    "# item_vecs  # 記事ベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7105,  0.7686, -0.2016],\n",
       "         [ 0.6952, -0.2221, -0.5135],\n",
       "         [ 2.4392,  0.9282, -0.2792],\n",
       "         [ 0.3483,  1.9796, -1.8862]],\n",
       "\n",
       "        [[ 2.4392,  0.9282, -0.2792],\n",
       "         [-2.4020, -0.7471, -0.2395],\n",
       "         [ 0.6952, -0.2221, -0.5135],\n",
       "         [ 0.7127,  0.9713,  1.0156]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = nn.Linear(1, 10)\n",
    "input = torch.randint(0, 10, (2, 1), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 2.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(input, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([333, 3, 1500])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = torch.ones(333, 3, 100), torch.ones(100, 1500)\n",
    "torch.matmul(a, b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([333, 100])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.ones(333, 3, 100)\n",
    "torch.mean(a, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([[[1, 2, 3], [3, 4, 5]], [[1, 2, 3], [3, 4, 5]], [[1, 2, 3], [3, 4, 5]], [[1, 2, 3], [3, 4, 5]]], dtype=torch.float)\n",
    "print(c.shape)\n",
    "torch.mean(c, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
